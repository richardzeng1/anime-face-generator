{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "420 Project.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDIJ8AhzLXGk",
        "colab_type": "code",
        "outputId": "1fca2dc0-701c-41f1-e3f2-6a5935f6bbac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "!rm -rf train\n",
        "!mkdir train\n",
        "import requests\n",
        "import random\n",
        "import json\n",
        "import shutil\n",
        "\n",
        "from IPython.display import HTML, display\n",
        "import time\n",
        "\n",
        "def progress(value, max=100):\n",
        "    return HTML(\"\"\"\n",
        "        <progress\n",
        "            value='{value}'\n",
        "            max='{max}',\n",
        "            style='width: 40%'\n",
        "        >\n",
        "            {value}\n",
        "        </progress>\n",
        "    \"\"\".format(value=value, max=max))\n",
        "\n",
        "def neko():\n",
        "    sources = [7, 20]\n",
        "    urls = []\n",
        "\n",
        "    print \"[Scrapping for neko images...]\"\n",
        "    for s in sources:\n",
        "        afterdate = None\n",
        "        for _ in xrange(150):\n",
        "            q = 'https://redditbooru.com/api/images/?sources=%s' % s\n",
        "            if afterdate is not None:\n",
        "                q += '&afterDate=%s' % afterdate\n",
        "            data = requests.get(q).json()\n",
        "            if data is None: continue\n",
        "            urls += list(filter(lambda d: not d['nsfw'], data))\n",
        "            afterdate = min(d['dateCreated'] for d in data)\n",
        "\n",
        "    print \"[Finished getting all URLs]\"\n",
        "    print \"[Got %d images]\\n\" % len(urls)\n",
        "    count = 0\n",
        "\n",
        "    print \"Downloading images...\"\n",
        "    out = display(progress(0, len(urls)), display_id=True)\n",
        "\n",
        "    for d in urls:\n",
        "        resp = requests.get(d['cdnUrl'], stream=True)\n",
        "        f = \"train/%d.jpg\" % count\n",
        "        count += 1\n",
        "        local_file = open(f, 'wb')\n",
        "\n",
        "        resp.raw.decode_content = True\n",
        "\n",
        "        shutil.copyfileobj(resp.raw, local_file)\n",
        "\n",
        "        out.update(progress(count, len(urls)))\n",
        "\n",
        "        del resp\n",
        "\n",
        "    print \"Finished downloading images\"\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    neko()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Scrapping for neko images...]\n",
            "[Finished getting all URLs]\n",
            "[Got 6525 images]\n",
            "\n",
            "Downloading images...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "        <progress\n",
              "            value='6525'\n",
              "            max='6525',\n",
              "            style='width: 40%'\n",
              "        >\n",
              "            6525\n",
              "        </progress>\n",
              "    "
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Finished downloading images\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQN-U1R0D5D1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!zip -r train-scaled.zip train-scaled"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nytKYeFM0gQF",
        "colab_type": "code",
        "outputId": "c285bb7e-23f4-4b40-d5c4-a54d55287035",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "!rm -rf train-scaled\n",
        "!mkdir train-scaled\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "\n",
        "sz = (512, 512)\n",
        "\n",
        "a = transforms.Compose([\n",
        "    transforms.Resize(512)\n",
        "])\n",
        "\n",
        "for i in xrange(1460, 6525):\n",
        "  f = \"train/%d.jpg\" % i\n",
        "\n",
        "  try:\n",
        "    img = Image.open(f)\n",
        "    img = a(img)\n",
        "    img = img.convert('RGB')\n",
        "  except Exception:\n",
        "    continue\n",
        "\n",
        "  f2 = \"train-scaled/%d.jpg\" % i\n",
        "  img.save(f2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/PIL/TiffImagePlugin.py:742: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 0. \n",
            "  warnings.warn(str(msg))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vO4QFSgaTD5I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip faceset.zip\n",
        "!mkdir dataset\n",
        "!mv faceset dataset/1\n",
        "#!mkdir results\n",
        "#!mkdir checkpoint"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0slSqwhriJkk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip train-scaled.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0gasv1IT1t-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import math\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms, utils\n",
        "\n",
        "import sys\n",
        "import torch\n",
        "from torch.utils.data import dataloader\n",
        "from torch.multiprocessing import reductions\n",
        "from multiprocessing.reduction import ForkingPickler\n",
        "\n",
        "from model import *\n",
        "\n",
        "default_collate_func = dataloader.default_collate\n",
        "\n",
        "\n",
        "def default_collate_override(batch):\n",
        "    dataloader._use_shared_memory = False\n",
        "    return default_collate_func(batch)\n",
        "\n",
        "setattr(dataloader, 'default_collate', default_collate_override)\n",
        "\n",
        "for t in torch._storage_classes:\n",
        "    if sys.version_info[0] == 2:\n",
        "        if t in ForkingPickler.dispatch:\n",
        "            del ForkingPickler.dispatch[t]\n",
        "    else:\n",
        "        if t in ForkingPickler._extra_reducers:\n",
        "            del ForkingPickler._extra_reducers[t]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVHnNVksMYff",
        "colab_type": "code",
        "outputId": "ec73eb95-ff5b-4507-a055-db8c4ef7509c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        }
      },
      "source": [
        "# use idel gpu\n",
        "# it's better to use enviroment variable\n",
        "# if you want to use multiple gpus, please\n",
        "# modify hyperparameters at the same time\n",
        "# And Make Sure Your Pytorch Version >= 1.0.1\n",
        "import os\n",
        "n_gpu             = 1\n",
        "device            = torch.device('cuda:0')\n",
        "\n",
        "# Original Learning Rate\n",
        "learning_rate   = {128: 0.0015, 256: 0.002, 512: 0.003, 1024: 0.003}\n",
        "# For anime only\n",
        "# learning_rate     = {512: 0.0015, 1024: 0.002}\n",
        "batch_size_1gpu   = {4: 128, 8: 128, 16: 64, 32: 32, 64: 16, 128: 16}\n",
        "mini_batch_size_1 = 8\n",
        "batch_size        = {4: 256, 8: 256, 16: 128, 32: 64, 64: 32, 128: 16}\n",
        "mini_batch_size   = 8\n",
        "batch_size_4gpus  = {4: 512, 8: 256, 16: 128, 32: 64, 64: 32}\n",
        "mini_batch_size_4 = 16\n",
        "batch_size_8gpus  = {4: 512, 8: 256, 16: 128, 32: 64}\n",
        "mini_batch_size_8 = 32\n",
        "# Commen line below if you don't meet the problem of 'shared memory conflict'\n",
        "num_workers       = {128: 8, 256: 4, 512: 2}\n",
        "max_workers       = 16\n",
        "n_fc              = 8\n",
        "dim_latent        = 512\n",
        "dim_input         = 4\n",
        "# number of samples to show before dowbling resolution\n",
        "n_sample          = 5500\n",
        "# number of samples train model in total\n",
        "n_sample_total    = 200_000\n",
        "DGR               = 1\n",
        "n_show_loss       = 360\n",
        "step              = 1 # Train from (8 * 8)\n",
        "max_step          = 5\n",
        "style_mixing      = [] # Waiting to implement\n",
        "image_folder_path = './dataset/'\n",
        "save_folder_path  = './results/'\n",
        "\n",
        "low_steps         = [0, 1, 2]\n",
        "# style_mixing    += low_steps\n",
        "mid_steps         = [3, 4, 5]\n",
        "# style_mixing    += mid_steps\n",
        "hig_steps         = [6, 7, 8]\n",
        "# style_mixing    += hig_steps\n",
        "\n",
        "# Used to continue training from last checkpoint\n",
        "iteration         = 0\n",
        "startpoint        = 0\n",
        "used_sample       = 0\n",
        "alpha             = 0\n",
        "\n",
        "# How to start training?\n",
        "# True for start from saved model\n",
        "# False for retrain from the very beginning\n",
        "is_continue       = True\n",
        "d_losses          = [float('inf')]\n",
        "g_losses          = [float('inf')]\n",
        "\n",
        "def set_grad_flag(module, flag):\n",
        "    for p in module.parameters():\n",
        "        p.requires_grad = flag\n",
        "\n",
        "def reset_LR(optimizer, lr):\n",
        "    for pam_group in optimizer.param_groups:\n",
        "        mul = pam_group.get('mul', 1)\n",
        "        pam_group['lr'] = lr * mul\n",
        "        \n",
        "# Gain sample\n",
        "def gain_sample(dataset, batch_size, image_size=4):\n",
        "    transform = transforms.Compose([\n",
        "            transforms.Resize(image_size),          # Resize to the same size\n",
        "            transforms.CenterCrop(image_size),      # Crop to get square area\n",
        "            transforms.RandomHorizontalFlip(),      # Increase number of samples\n",
        "            transforms.ToTensor(),            \n",
        "            transforms.Normalize((0.5, 0.5, 0.5),\n",
        "                                 (0.5, 0.5, 0.5))])\n",
        "\n",
        "    dataset.transform = transform\n",
        "    loader = DataLoader(dataset, shuffle=True, batch_size=batch_size, num_workers=num_workers.get(image_size, max_workers))\n",
        "\n",
        "    return loader\n",
        "\n",
        "def imsave(tensor, i):\n",
        "    grid = tensor[0]\n",
        "    grid.clamp_(-1, 1).add_(1).div_(2)\n",
        "    # Add 0.5 after normalizing to [0, 255] to round to nearest integer\n",
        "    ndarr = grid.mul_(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).to('cpu', torch.uint8).numpy()\n",
        "    img = Image.fromarray(ndarr)\n",
        "    img.save(f'{save_folder_path}sample-iter{i}.png')\n",
        "    \n",
        "# Train function\n",
        "def train(generator, discriminator, g_optim, d_optim, dataset, step, iteration=0, startpoint=0, used_sample=0,\n",
        "         d_losses = [], g_losses = [], alpha=0):\n",
        "    \n",
        "    resolution  = 4 * 2 ** step\n",
        "    \n",
        "    origin_loader = gain_sample(dataset, batch_size.get(resolution, mini_batch_size), resolution)\n",
        "    data_loader = iter(origin_loader)\n",
        "    \n",
        "    reset_LR(g_optim, learning_rate.get(resolution, 0.001))\n",
        "    reset_LR(d_optim, learning_rate.get(resolution, 0.001))\n",
        "    progress_bar = tqdm(total=n_sample_total, initial=used_sample)\n",
        "    # Train\n",
        "    while used_sample < n_sample_total:\n",
        "        iteration += 1\n",
        "        alpha = min(1, alpha + batch_size.get(resolution, mini_batch_size) / (n_sample))\n",
        "        \n",
        "        if (used_sample - startpoint) > n_sample and step < max_step: \n",
        "            step += 1\n",
        "            alpha = 0\n",
        "            startpoint = used_sample\n",
        "            \n",
        "            resolution = 4 * 2 ** step\n",
        "            \n",
        "            # Avoid possible memory leak\n",
        "            del origin_loader\n",
        "            del data_loader\n",
        "            \n",
        "            # Change batch size\n",
        "            origin_loader = gain_sample(dataset, batch_size.get(resolution, mini_batch_size), resolution)\n",
        "            data_loader = iter(origin_loader)\n",
        "            \n",
        "            reset_LR(g_optim, learning_rate.get(resolution, 0.001))\n",
        "            reset_LR(d_optim, learning_rate.get(resolution, 0.001))\n",
        "            \n",
        "        \n",
        "        try:\n",
        "            # Try to read next image\n",
        "            real_image, label = next(data_loader)\n",
        "\n",
        "        except (OSError, StopIteration):\n",
        "            # Dataset exhausted, train from the first image\n",
        "            data_loader = iter(origin_loader)\n",
        "            real_image, label = next(data_loader)\n",
        "        \n",
        "        # Count used sample\n",
        "        used_sample += real_image.shape[0]\n",
        "        progress_bar.update(real_image.shape[0])\n",
        "        \n",
        "        # Send image to GPU\n",
        "        real_image = real_image.to(device)\n",
        "        \n",
        "        # D Module ---\n",
        "        # Train discriminator first\n",
        "        discriminator.zero_grad()\n",
        "        set_grad_flag(discriminator, True)\n",
        "        set_grad_flag(generator, False)\n",
        "        \n",
        "        # Real image predict & backward\n",
        "        # We only implement non-saturating loss with R1 regularization loss\n",
        "        real_image.requires_grad = True\n",
        "        if n_gpu > 1:\n",
        "            real_predict = nn.parallel.data_parallel(discriminator, (real_image, step, alpha), range(n_gpu))\n",
        "        else:\n",
        "            real_predict = discriminator(real_image, step, alpha)\n",
        "        real_predict = nn.functional.softplus(-real_predict).mean()\n",
        "        real_predict.backward(retain_graph=True)\n",
        "\n",
        "        grad_real = torch.autograd.grad(outputs=real_predict.sum(), inputs=real_image, create_graph=True)[0]\n",
        "        grad_penalty_real = (grad_real.view(grad_real.size(0), -1).norm(2, dim=1) ** 2).mean()\n",
        "        grad_penalty_real = 10 / 2 * grad_penalty_real\n",
        "        grad_penalty_real.backward()\n",
        "        \n",
        "        # Generate latent code\n",
        "        latent_w1 = [torch.randn((batch_size.get(resolution, mini_batch_size), dim_latent), device=device)]\n",
        "        latent_w2 = [torch.randn((batch_size.get(resolution, mini_batch_size), dim_latent), device=device)]\n",
        "\n",
        "        noise_1 = []\n",
        "        noise_2 = []\n",
        "        for m in range(step + 1):\n",
        "            size = 4 * 2 ** m # Due to the upsampling, size of noise will grow\n",
        "            noise_1.append(torch.randn((batch_size.get(resolution, mini_batch_size), 1, size, size), device=device))\n",
        "            noise_2.append(torch.randn((batch_size.get(resolution, mini_batch_size), 1, size, size), device=device))\n",
        "        \n",
        "        # Generate fake image & backward\n",
        "        if n_gpu > 1:\n",
        "            fake_image = nn.parallel.data_parallel(generator, (latent_w1, step, alpha, noise_1), range(n_gpu))\n",
        "            fake_predict = nn.parallel.data_parallel(discriminator, (fake_image, step, alpha), range(n_gpu))\n",
        "        else:\n",
        "            fake_image = generator(latent_w1, step, alpha, noise_1)\n",
        "            fake_predict = discriminator(fake_image, step, alpha)\n",
        "\n",
        "        fake_predict = nn.functional.softplus(fake_predict).mean()\n",
        "        fake_predict.backward()\n",
        "        \n",
        "        if iteration % n_show_loss == 0:\n",
        "            d_losses.append((real_predict + fake_predict).item())\n",
        "        \n",
        "        # D optimizer step\n",
        "        d_optim.step()\n",
        "        \n",
        "        # Avoid possible memory leak\n",
        "        del grad_penalty_real, grad_real, fake_predict, real_predict, fake_image, real_image, latent_w1\n",
        "                   \n",
        "        # G module ---\n",
        "        if iteration % DGR != 0: continue\n",
        "        # Due to DGR, train generator\n",
        "        generator.zero_grad()\n",
        "        set_grad_flag(discriminator, False)\n",
        "        set_grad_flag(generator, True)\n",
        "        \n",
        "        if n_gpu > 1:\n",
        "            fake_image = nn.parallel.data_parallel(generator, (latent_w2, step, alpha, noise_2), range(n_gpu))\n",
        "            fake_predict = nn.parallel.data_parallel(discriminator, (fake_image, step, alpha), range(n_gpu))\n",
        "        else: \n",
        "            fake_image = generator(latent_w2, step, alpha, noise_2)\n",
        "            fake_predict = discriminator(fake_image, step, alpha)\n",
        "        fake_predict = nn.functional.softplus(-fake_predict).mean()\n",
        "        fake_predict.backward()\n",
        "        g_optim.step()\n",
        "\n",
        "        if iteration % n_show_loss == 0:\n",
        "            g_losses.append(fake_predict.item())\n",
        "            imsave(fake_image.data.cpu(), iteration)\n",
        "            \n",
        "        # Avoid possible memory leak\n",
        "        del fake_predict, fake_image, latent_w2\n",
        "        \n",
        "        if iteration % 1000 == 0:\n",
        "            # Save the model every 1000 iterations\n",
        "            torch.save({\n",
        "                'generator'    : generator.state_dict(),\n",
        "                'discriminator': discriminator.state_dict(),\n",
        "                'g_optim'      : g_optim.state_dict(),\n",
        "                'd_optim'      : d_optim.state_dict(),\n",
        "                'parameters'   : (step, iteration, startpoint, used_sample, alpha),\n",
        "                'd_losses'     : d_losses,\n",
        "                'g_losses'     : g_losses\n",
        "            }, 'checkpoint/trained.pth')\n",
        "            print(f'Model successfully saved.')\n",
        "        \n",
        "        progress_bar.set_description((f'Resolution: {resolution}*{resolution}  D_Loss: {d_losses[-1]:.4f}  G_Loss: {g_losses[-1]:.4f}  Alpha: {alpha:.4f}'))\n",
        "    torch.save({\n",
        "        'generator'    : generator.state_dict(),\n",
        "        'discriminator': discriminator.state_dict(),\n",
        "        'g_optim'      : g_optim.state_dict(),\n",
        "        'd_optim'      : d_optim.state_dict(),\n",
        "        'parameters'   : (step, iteration, startpoint, used_sample, alpha),\n",
        "        'd_losses'     : d_losses,\n",
        "        'g_losses'     : g_losses\n",
        "    }, 'checkpoint/trained.pth')\n",
        "    print(f'Final model successfully saved.')\n",
        "    return d_losses, g_losses\n",
        "\n",
        "\n",
        "# generator      = nn.DataParallel(StyleBased_Generator(n_fc, dim_latent, dim_input)).cuda()\n",
        "# discriminator  = nn.DataParallel(Discriminator()).cuda()  \n",
        "# g_optim        = optim.Adam([{\n",
        "#     'params': generator.module.convs.parameters(),\n",
        "#     'lr'    : 0.001\n",
        "# }, {\n",
        "#     'params': generator.module.to_rgbs.parameters(),\n",
        "#     'lr'    : 0.001\n",
        "# }], lr=0.001, betas=(0.0, 0.99))\n",
        "# g_optim.add_param_group({\n",
        "#     'params': generator.module.fcs.parameters(),\n",
        "#     'lr'    : 0.001 * 0.01,\n",
        "#     'mul'   : 0.01\n",
        "# })\n",
        "\n",
        "# Create models\n",
        "generator      = StyleBased_Generator(n_fc, dim_latent, dim_input).to(device)\n",
        "discriminator  = Discriminator().to(device)\n",
        "\n",
        "# Optimizers\n",
        "g_optim        = optim.Adam([{\n",
        "    'params': generator.convs.parameters(),\n",
        "    'lr'    : 0.001\n",
        "}, {\n",
        "    'params': generator.to_rgbs.parameters(),\n",
        "    'lr'    : 0.001\n",
        "}], lr=0.001, betas=(0.0, 0.99))\n",
        "g_optim.add_param_group({\n",
        "    'params': generator.fcs.parameters(),\n",
        "    'lr'    : 0.001 * 0.01,\n",
        "    'mul'   : 0.01\n",
        "})\n",
        "d_optim        = optim.Adam(discriminator.parameters(), lr=0.001, betas=(0.0, 0.99))\n",
        "dataset        = datasets.ImageFolder(image_folder_path)\n",
        "\n",
        "if is_continue:\n",
        "    if os.path.exists('checkpoint/trained.pth'):\n",
        "        # Load data from last checkpoint\n",
        "        print('Loading pre-trained model...')\n",
        "        checkpoint = torch.load('checkpoint/trained.pth')\n",
        "        generator.load_state_dict(checkpoint['generator'])\n",
        "        discriminator.load_state_dict(checkpoint['discriminator'])\n",
        "        g_optim.load_state_dict(checkpoint['g_optim'])\n",
        "        d_optim.load_state_dict(checkpoint['d_optim'])\n",
        "        step, iteration, startpoint, used_sample, alpha = checkpoint['parameters']\n",
        "        d_losses = checkpoint.get('d_losses', [float('inf')])\n",
        "        g_losses = checkpoint.get('g_losses', [float('inf')])\n",
        "        print('Start training from loaded model...')\n",
        "    else:\n",
        "        print('No pre-trained model detected, restart training...')\n",
        "        \n",
        "if 1:\n",
        "  generator.train()\n",
        "  discriminator.train()    \n",
        "  d_losses, g_losses = train(generator, discriminator, g_optim, d_optim, dataset, step, iteration, startpoint, used_sample, d_losses, g_losses, alpha)\n",
        "else:\n",
        "  generator.eval()\n",
        "  #x = evaluate(compute_latent_cernter(), np.array([0.001]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading pre-trained model...\n",
            "Start training from loaded model...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Resolution: 128*128  D_Loss: 0.1947  G_Loss: 4.0580  Alpha: 1.0000:  56%|█████▋    | 112844/200000 [1:11:16<9:01:33,  2.68it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model successfully saved.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Resolution: 128*128  D_Loss: 0.4309  G_Loss: 5.6489  Alpha: 1.0000:  64%|██████▍   | 128829/200000 [2:40:01<7:18:03,  2.71it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model successfully saved.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Resolution: 128*128  D_Loss: 0.6010  G_Loss: 6.3934  Alpha: 1.0000:  72%|███████▏  | 144814/200000 [4:08:35<5:39:21,  2.71it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model successfully saved.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Resolution: 128*128  D_Loss: 0.0765  G_Loss: 4.6335  Alpha: 1.0000:  80%|████████  | 160799/200000 [5:37:11<4:05:08,  2.67it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model successfully saved.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Resolution: 128*128  D_Loss: 0.1180  G_Loss: 3.9576  Alpha: 1.0000:  88%|████████▊ | 176784/200000 [7:06:03<2:23:13,  2.70it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model successfully saved.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Resolution: 128*128  D_Loss: 0.0151  G_Loss: 6.7672  Alpha: 1.0000:  96%|█████████▋| 192769/200000 [8:34:56<44:38,  2.70it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model successfully saved.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Resolution: 128*128  D_Loss: 0.0035  G_Loss: 7.6371  Alpha: 1.0000: : 200012it [9:15:13,  3.00it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Final model successfully saved.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbhnsvOPQlNt",
        "colab_type": "code",
        "outputId": "1089e251-9f1b-41df-8c0a-8ed2d8139fe7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZilwfjyKpHX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r checkpoint drive/My\\ Drive/420proj"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofdEOpshK4Mp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r results drive/My\\ Drive/420proj"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irMuzMcwxtK-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r drive/My\\ Drive/420proj/checkpoint ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65wPKul8x474",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r drive/My\\ Drive/420proj/results ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99hj843C7tfV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r drive/My\\ Drive/420proj/faceset.zip ."
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}